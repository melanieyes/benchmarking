# ARC Benchmark Evaluation: GPT-4.1-nano vs. GPT-4o-mini

This repository contains a comprehensive evaluation solution for the **Benchmarking Challenge**. It evaluates and compares the performance of `GPT-4.1-nano` and `GPT-4o-mini` on the **ARC-Challenge** dataset (2,590 questions).

The project implements a custom evaluation harness to handle log-probability extraction, few-shot prompting sweeps, and confidence calibration analysis.

## ðŸŽ¯ Project Scope

This solution addresses all three levels of the challenge:

* **Basic**: Evaluation of `GPT-4.1-nano` on the ARC-Challenge dataset.
* **Moderate**: Comparative analysis between `GPT-4.1-nano` and `GPT-4o-mini`, including a paired analysis of questions where models diverge.
* **Advanced**: A custom log-probability based evaluation. This includes:
    * Extracting logprobs via the OpenAI API (working around standard harness limitations).
    * Normalizing probabilities over the valid choice set (A, B, C, D).
    * Measuring confidence vs. accuracy across varying few-shot ($k$) examples.

## ðŸ“‚ Repository Structure

* **`arc_eval.py`**: The main evaluation script.
    * Fetches data from HuggingFace (`allenai/ai2_arc`).
    * Runs the evaluation sweep (0-shot to 8-shot).
    * Implements robust retries to extract logprobs for specific answer tokens.
    * Saves results to local CSV files.
* **`arcbench.ipynb`**: Analysis notebook.
    * Loads the CSV results generated by the script.
    * Calculates accuracy and calibration metrics.
    * Performs paired error analysis (Both Wrong, Only Nano Correct, etc.).
    * Visualizes the relationship between model confidence and few-shot examples.
* **`.gitignore`**: Configured to exclude large CSV data files from the repository.

## ðŸš€ Setup & Installation

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/melanieyes/benchmarking.git](https://github.com/melanieyes/benchmarking.git)
    cd benchmarking
    ```

2.  **Install dependencies:**
    ```bash
    pip install openai datasets pandas numpy tqdm python-dotenv matplotlib
    ```

3.  **Environment Setup:**
    Create a `.env` file in the root directory and add your OpenAI API key:
    ```text
    OPENAI_API_KEY=your_api_key_here
    ```

## ðŸ“Š Usage

### 1. Run the Evaluation
Since the CSV data files are not included in the repo (to keep it light), you must generate them first. This script runs the evaluation for both models across multiple shot counts ($k=0, 1, 2, 4, 8$).

```bash
python arc_eval.py
